{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Downloading stopwords'''\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Function to tokenize, remove stop words and create a bigram model'''\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "def process_message(message):\n",
    "    gram=2\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    try:\n",
    "        word_tokens = word_tokenize(message) \n",
    "    except:\n",
    "        return\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "    filtered_sentence = [] \n",
    "    for w in word_tokens: \n",
    "        if w not in stop_words: \n",
    "            filtered_sentence.append(w) \n",
    "    words=[w for w in filtered_sentence if len(w)>2]\n",
    "    w=[]\n",
    "    for i in range(len(words)-gram+1):\n",
    "        w+=[''.join(words[i:i+gram])]\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5572"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Read the dataset'''\n",
    "import pandas as pd\n",
    "df=pd.read_csv('spam.csv')\n",
    "df.v2 = df.v2.astype(str).str.lower()\n",
    "X=df.iloc[:, 1].values\n",
    "y=df.iloc[:,0].values\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder=LabelEncoder()\n",
    "y=label_encoder.fit_transform(y)\n",
    "y.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5324"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Processing the dataset'''\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "numcol=X.size\n",
    "X_processed=[]\n",
    "bigram_set=set()\n",
    "y_list=list()\n",
    "for i in range (numcol):\n",
    "    pm=process_message(X[i])\n",
    "    if pm:\n",
    "        y_list.append(y[i])\n",
    "        for j in range(len(pm)):\n",
    "            bigram_set.add(pm[j]) \n",
    "        X_processed.append(Counter(pm))\n",
    "y_new=np.asarray(y_list)\n",
    "y_new.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Creating bag of words of the bigram model'''\n",
    "rows=len(X_processed)\n",
    "cols=len(bigram_set)\n",
    "matrix=np.zeros(shape=(rows,cols))\n",
    "for i in range (rows):\n",
    "    count=0\n",
    "    for j in bigram_set:\n",
    "        if j in X_processed[i]:\n",
    "            #print(X_processed[i][j] )\n",
    "            matrix[i][count]=X_processed[i][j]\n",
    "        #print(matrix[i][count])    \n",
    "        count+=1\n",
    "matrix      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\Anaconda2\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "'''Splitting dataset into training and testing data'''\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(matrix, y_new, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3726x29678 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 28764 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Compressing the sparse matrix'''\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.sparse import csr_matrix\n",
    "sparse_dataset = csr_matrix(X_train)\n",
    "sparse_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8892365456821026"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''SVM Classifier'''\n",
    "from sklearn.svm import SVC\n",
    "model=SVC(gamma=2, C=1)\n",
    "model.fit(sparse_dataset, y_train)\n",
    "pr=model.predict(X_test)\n",
    "score = accuracy_score(y_test, pr)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.95989305 0.94919786 0.96782842 0.96246649 0.94354839 0.96774194\n",
      " 0.95698925 0.9516129  0.95967742 0.9516129 ]\n"
     ]
    }
   ],
   "source": [
    "'''Decision Tree Classifier'''\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model=DecisionTreeClassifier()\n",
    "#model.fit(sparse_dataset, y_train)\n",
    "#pr=model.predict(X_test)\n",
    "#score = accuracy_score(y_test, pr)\n",
    "from sklearn.model_selection import cross_val_score\n",
    "print(cross_val_score(model, sparse_dataset, y_train, cv = 10, scoring = \"accuracy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\Anaconda2\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.94385027, 0.92245989, 0.94906166, 0.95710456, 0.92741935,\n",
       "       0.94892473, 0.92204301, 0.93548387, 0.94892473, 0.93010753])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Random Forest Classifier'''\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model=RandomForestClassifier()\n",
    "model.fit(sparse_dataset, y_train)\n",
    "pr=model.predict(X_test)\n",
    "score = accuracy_score(y_test, pr)\n",
    "score\n",
    "cross_val_score(model, sparse_dataset, y_train, cv = 10, scoring = \"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8066332916145181"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Naive Bayes Classifier'''\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes.fit(sparse_dataset, y_train)\n",
    "y_predict=naive_bayes.predict(X_test)\n",
    "score = accuracy_score(y_test, y_predict)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8873591989987485"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Gradient Boost Classifier'''\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "model= GradientBoostingClassifier(learning_rate=0.03,random_state=1)\n",
    "model.fit(sparse_dataset, y_train)\n",
    "pr=model.predict(X_test)\n",
    "score = accuracy_score(y_test, pr)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
